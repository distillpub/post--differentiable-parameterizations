<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "A powerful, under-explored tool for neural network visualizations and art.",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google AI",
      "affiliationURL": "https://research.google.com/"
    },
    {
      "author": "Nicola Pezzotti",
      "authorURL": "https://nicola17.github.io/",
      "affiliation": "Google AI",
      "affiliationURL": "https://research.google.com/"
    },
    {
      "author": "Ludwig Schubert",
      "authorURL": "https://schubert.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
  <p>A powerful, under-explored tool for neural network visualizations and art.</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>


  <p id="first-paragraph">
    Neural networks trained to classify images have a remarkable -- and surprising! -- capacity to generate images.
    Techniques such as DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite>, style transfer<d-cite key="gatys2015"></d-cite>, and feature visualization<d-cite key="olah2017feature"></d-cite> leverage this capacity as a powerful tool for exploring the inner workings of neural networks, and to fuel a small artistic movement based on neural art.
  </p>

  <p>
    All these techniques work in roughly the same way.
    Neural networks used in computer vision have a rich internal representation of the images they look at.
    We can use this representation to describe the properties we want an image to have (e.g. style), and then optimize the input image to have those properties.
    This kind of optimization is possible because the networks are differentiable with respect to their inputs: we can slightly tweak the image to better fit the desired properties, and then iteratively apply such tweaks in gradient descent.
  </p>

  <p>
    Typically, we parameterize the input image as the RGB values of each pixel, but that isn’t the only way.
    As long as the mapping from parameters to images is differentiable, we can still optimize alternative parameterizations with gradient descent.
  </p>

  <figure class="subgrid" id="figure-differentiable-parameterizations">
    <figcaption style="grid-column: kicker">
      <p>
        <a href="#figure-differentiable-parameterizations" class="figure-number">1</a>:
        <span style="hyphens: manual;">As long as an </span>
        <span style="background-color: #FFF1E7; padding-left: 2px; padding-right: 2px; border-radius: 2px;">image para&shy;meter&shy;ization</span>
        <span>is differ&shy;entiable, we can back&shy;propagate</span>
        <span style="white-space: nowrap;">( <span style="display: inline-block; width: 27px; height: 9px; margin-left: -2px; margin-right: -1px;"> <%= require("../static/images/diagrams/backprop-arrow.svg") %> </span> )</span>
        <span>through it.</span>
      </p>
    </figcaption>
    <div class="l-body">
      <%= require("../static/images/diagrams/general.svg") %>
    </div>
  </figure>

  <p>

    Differentiable image parameterizations invite us to ask "what kind of image generation process can we backpropagate through?"
    The answer is quite a lot, and some of the more exotic possibilities can create a wide range of interesting effects, including 3D neural art, images with transparency, and aligned interpolation.
    Previous work using specific unusual image parameterizations <d-cite key="Nguyen2015:easilyFooled,athalye2017synthesizing,olah2017feature"></d-cite> has shown exciting results -- we think that zooming out and looking at this area as a whole suggests there's even more potential.
  </p>


  <!-- <aside>For each demonstration, we provide a colab notebook so that you can easily reproduce and build on our results.</aside> -->
<!--
  <p>
    Alternative parameterizations can achieve a lot of interesting effects, including 3D neural art, images with transparency, and aligned interpolation.
    We believe
    While recent work has begun to explore some of these possibilities [], we think that there’s a lot of untapped potential
  </p>
-->

<h3 id="why-does-parameterization-matter">Why Does Parameterization Matter?</h3>


<p>
  It may seem surprising that changing the parameterization of an optimization problem can significantly change the result, despite the objective function that is actually being optimized remaining the same.
  We see four reasons why the choice of parameterization can have a significant effect:
</p>

<p><b>(1) - Improved Optimization</b> -
Transforming the input to make an optimization problem easier -- a technique called "preconditioning" -- is a staple of optimization.
  <d-footnote>
    Preconditioning is most often presented as a transformation of the gradient
     (usually multiplying it by a positive definite "preconditioner" matrix).
    However, this is equivalent to optimizing an alternate parameterization of the input.
  </d-footnote>
We find that simple changes in parameterization make image optimization for neural art and image optimization much easier.
</p>

<p><b>(2) - Basins of Attraction</b> -
When we optimize the input to a neural network, there are often many different solutions, corresponding to different local minima.
  <d-footnote>
    Training deep neural networks characterized by complex optimization landscapes <d-cite key="choromanska2015loss"></d-cite>, which may have many equally good local minima for a given objective.
    (Note that finding the global minimum is not always desirable as it may result in an overfitted model <d-cite key="choromanska2015loss"></d-cite>.)
    Thus, it's probably not surprising that optimizing the input to a neural network would also have many local minima.
  </d-footnote>
The probability of our optimization process falling into any particular local minima is controlled by its basin of attraction (i.e., the region of the optimization landscape under the influence of the minimum).
Changing the parameterization of an optimization problem is known to change the sizes of different basins of attraction, influencing the likely result.
</p>

<p><b>(3) - Additional Constraints </b> -
Some parameterizations cover only a subset of possible inputs, rather than the entire space.
An optimizer working in such a parameterization will still find solutions that minimize or maximize the objective function, but they'll be subject to the constraints of the parameterization.
By picking the right set of constraints, one can impose a variety of constraints, ranging from simple constraints (e.g., the boundary of the image must be black), to rich, subtle constraints.

<p><b>(4) - Implicitly Optimizing other Objects</b> -
  A parameterization may internally use a different kind of object than the one it outputs and we optimize for.
  For example, while the natural input to a vision network is an RGB image, we can parameterize that image as a rendering of a 3D object and, by backpropagating through the rendering process, optimize that instead.
  Because the 3D object has more degrees of freedom than the image, we generally use a <em>stochastic</em> parameterization that produces images rendered from different perspectives.</p>

<p>
In the rest of the article we give concrete examples where such approaches are beneficial and lead to surprising and interesting visual results.
</p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-aligned-interpolation" class="section-number">1</a>
  </p>
  <h2 id="section-aligned-interpolation"><a href="#section-aligned-interpolation">Aligned Feature Visualization Interpolation</a></h2>

<!--
  <p>
As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
Throughout this article we will often optimize a randomly initialized parameterization to generate the patterns that are detected by a neurons, channels or layers in a neural network.
We refer to the result of this procedure as the optimization for a feature visualizations objective function<d-cite key="olah2017feature"></d-cite>, as it reveals the features that the network is detecting in different layers.
  </p>
-->

<p>
  Feature visualization is most often used to visualize individual neurons,
  but it can also be used to <a href="https://distill.pub/2017/feature-visualization/#interaction">visualize combinations of neurons</a>, in order to study how they interact <d-cite key="olah2017feature"></d-cite>.
  Instead of optimizing an image to make a single neuron fire, one optimizes it to make multiple neurons fire.
</p>

<p>
  When we want to really understand the interaction between two neurons,
  we can go a step further and create multiple visualizations,
  gradually shifting the objective from optimizing one neuron to putting more weight on the other neuron firing.
  This is in some ways similar to interpolation in the latent spaces of generative models like GANs.
</p>

<p>
  Despite this, there is a small challenge: feature visualization is stochastic.
  Even if you optimize for the exact same objective, the visualization will be laid out differently each time.
  Normally, this isn’t a problem, but it does detract from the interpolation visualizations.
  If we make them naively, the resulting visualizations will be <i>unaligned</i>:
  visual landmarks such as eyes appear in different locations in each image.
  This lack of alignment can make it harder to see the difference due to slightly different objectives,
  because they're swamped by the much larger differences in layout.
</p>

<p>
  We can see the issue with independent optimization if we look at the interpolated frames as an animation:
</p>

  <!-- <img class="pointer" src="images/pointer.svg"> -->
  <span id="figure-number-aligned-interpolation-comparison" class="figcaption kicker-text-align add-colab-link--section-aligned-interpolation" style="grid-column: kicker; margin-top: 20px;">
    <p><a href="#figure-aligned-interpolation-comparison" class="figure-number">2</a></p>
  </span>
  <d-figure id='AlignedUnalignedComparison'></d-figure>

  <!-- <figure class="">
    <div style="margin-bottom: 1em;" id='UnalignedInterpolation'></div>
    <figcaption>
      Even though all optimized for the same objective, visual features such as eyes appear in different locations in these visualizations.
      This happens because each image starts the optimization from a different, random initialization.
    </figcaption>
  </figure> -->

  <p>
    How can we achieve this aligned interpolation, where visual landmarks do not move between frames?
    There are a number of possible approaches one could try
    <d-footnote>
      For example, one could explicitly penalize differences between adjacent frames. Our final result and our colab notebook use this technique in combination with a shared parameterization.
    </d-footnote>
    , one of which is using a <em>shared parameterization</em>: each frame is parameterized as a combination of its own unique parameterization, and a single shared one.
    <!-- The shared parameterization makes it easier for the optimization process to find frames in which visual landmarks stay in the same position then frames where they don't. In order to produce an unaligned frame, this frame's unique parameterization would have to overpower the shared parameterization. -->
  </p>

  <span class="figcaption kicker-text-align add-colab-link--section-aligned-interpolation" style="grid-column: kicker; margin-top: 20px;">
    <p><a href="#figure-aligned-interpolation-examples" class="figure-number">3</a></p>
  </span>
  <d-figure class="distill-experimental-autoresize" id='AlignedInterpolationExamples'></d-figure>

  <p>
    By partially sharing a parameterization between frames, we encourage the resulting visualizations to naturally align.
    Intuitively, the shared parameterization provides a common reference for the displacement of visual landmarks, while the unique one gives to each frame its own visual appeal based on its interpolation weights.
    <d-footnote>
      Concretely, we combine a usually lower-resolution shared parameterization $ P_{\text{shared}}$ and full-resolution independent parameterizations $P_{\text{unique}}^i$ that are unique to each frame $i$ of the visualization.
      Each individual frame $i$ is then parameterized as a combination $P^i$ of the two, $P^i = \sigma(P_{\text{unique}}^i + P_{\text{shared}})$, where $\sigma$ is the logistic sigmoid function.
    </d-footnote>
    This parameterization doesn't change the objective, but it does enlarge the <b>(2) basins of attraction</b> where the visualizations are aligned.
    <d-footnote>
      We can explicitly visualize how shared parameterization affects the basins of attraction in a toy example.
      Let's consider optimizing two variables $x$ and $y$ to both minimize $L(z)= (z^2-1)^2$.
      Since $L(z)$ has two basins of attraction $z=1$ or $z=-1$, the pair of optimization problems has four solutions:
      $(x,y) = (1,1)$, $(x,y) = (-1,1)$, $(x,y) = (1,-1)$, or $(x,y) = (-1,-1)$.
      Let's consider randomly initializing $x$ and $y$, and then optimizing them.
      Normally, the optimization problems are independent, so $x$ and $y$ are equally likely to come to unaligned solutions (where they have different signs) as aligned ones.
      But if we add a shared parameterization, the problems become coupled and the basin of attraction where they're aligned becomes bigger.<br>
      <img style="width: 100%; margin-top: 15px; max-width: 500px;" src="images/diagrams/basin-alignment.png"/>
    </d-footnote>
  </p>

  <p>
    This is an initial example of how differentiable parameterizations in general can be a useful additional tool in visualizing neural networks.
  </p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-styletransfer" class="section-number">2</a>
  </p>
  <h2 id='section-styletransfer'><a href="#section-styletransfer">Style Transfer with non-VGG architectures</a></h2>

  <p>
    Neural style transfer has a mystery:
    despite its remarkable success, almost all style transfer is done with variants of the <b>VGG architecture</b><d-cite key="simonyan2014vgg"></d-cite>.
    This isn't because no one is interested in doing style transfer on other architectures, but because attempts to do it on other architectures consistently work poorly.
    <d-footnote>
      Examples of experiments performed with different architectures can be found on <a href="https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8">Medium</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/">Reddit</a> and <a href="https://twitter.com/hardmaru/status/954173051330904065">Twitter</a>.
    </d-footnote>
    <!--This has practical implications: for example, if we could get style transfer working on smaller networks, using it in mobile applications would become more feasible.-->
  </p>

  <p>
    Several hypotheses have been proposed to explain why VGG works so much better than other models.
    One suggested explanation is that VGG's large size causes it to capture information that other models discard.
    This extra information, the hypothesis goes, isn't helpful for classification, but it does cause the model to work better for style transfer.
    An alternate hypothesis is that other models downsample more aggressively than VGG, losing spatial information.
    We suspect that there may be another factor: most modern vision models have checkerboard artifacts in their gradient <d-cite key="odena2016deconvolution,olah2017feature"></d-cite>, which could make optimization of the stylized image more difficult.
  </p>

  <p>
    In previous work we found that a <a href="//distill.pub/2017/feature-visualization/#preconditioning">decorrelated parameterization can significantly improve optimization</a><d-cite key="olah2017feature"></d-cite>.
    We find the same approach also improves style transfer, allowing us to use a model that did not otherwise produce visually appealing style transfer results:
  </p>

  <d-figure id='StyleTransferExamples' class="base-grid"></d-figure>

  <p id="figcaption--style-transfer-examples" class="figcaption l-body add-colab-link--section-styletransfer">
    <a href="#figure-style-transfer-examples" class="figure-number">4</a>:
    Move the slider under "final image optimization" to compare optimization in pixel space with optimization in a decorrelated space. Both images were created with the same objective and differ only in their parameterization.
  </p>

  <p>
    Let's consider this change in a bit more detail. Style transfer involves three images: a content image, a style image, and the image we optimize.
    All three of these feed into the CNN, and the style transfer objective<d-cite key="gatys2015"></d-cite> is based on the differences in how these images activate the CNN.
    The only change we make is how we parameterize the optimized image. Instead of parameterizing it in terms of pixels (which are highly correlated with their neighbors), we use a scaled Fourier transform<d-cite key="olah2017feature"></d-cite>.
  </p>

  <figure id="figure-style-transfer-diagram" class="subgrid">
    <!-- <img src="images/diagrams/styletransfer.png"/> -->
    <figcaption class="kicker-text-align" style="grid-column: kicker;">
      <a href="#figure-style-transfer-diagram" class="figure-number">5</a>
      <!-- Style Transfer optimizes an image for two objectives: a <strong>content objective</strong>, which aims to get neurons to fire in the same position as they did for the content image, and a <strong>style objective</strong>, which aims to create similar patterns of neuron activation anywhere as in the style image without regard to position.
      Normally, this process is sensitive to the choice of model. By parameterizing the optimized image in a decorrelated space it works for many more models. -->
    </figcaption>
    <div style="grid-column: text-start / page-end;">
      <%= require("../static/images/diagrams/styletransfer.svg") %>
    </div>
  </figure>

  <p>
    Our exact implementation can be found in the accompanying notebook. Note that it also uses <a href="https://distill.pub/2017/feature-visualization/#regularizer-playground-robust">transformation robustness</a><d-cite key="olah2017feature"></d-cite>, which not all implementations of style transfer use.
  </p>


  <!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-xy2rgb" class="section-number">3</a>
  </p>
  <h2 id='section-xy2rgb'><a href="#section-xy2rgb">Compositional Pattern Producing Networks</a></h2>

  <p>
    So far, we’ve explored image parameterizations that are relatively close to how we normally think of images, using pixels or Fourier components.
    In this section, we explore the possibility of <b> (3) adding additional constraints</b> to the optimization process by using a different parameterization.
    More specifically, we parameterize our image as a neural network <d-cite key="ulyanov2017deepimageprior"></d-cite> -- in particular, a Compositional Pattern Producing Network (CPPN) <d-cite key="stanley2007cppn"></d-cite>.
  </p>

  <p>
    CPPNs are neural networks that map $(x,y)$ positions to image colors:
  </p>
  <div class="math-overflow" style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
    $(x,y) ~\xrightarrow{\tiny CPPN}~ (r,g,b)$
  </div>
  <p>
    By applying the CPPN to a grid of positions, one can make arbitrary resolution images.
    The parameters of the CPPN network -- the weights and biases -- determine what image is produced.
    Depending on the architecture chosen for the CPPN, pixels in the resulting image are constrained to share, up to a certain degree, the color of their neighbors.
  </p>

  <p>
    Random parameters can produce aesthetically interesting images <d-cite key="CPPN:random"></d-cite>, but we can produce more interesting images by learning the parameters of the CPPN <d-cite key="karpathy2014cppns,ha2016cppns"></d-cite>.
    Often this is done by evolution  <d-cite key="sims1991evolution,stanley2007cppn,Nguyen2015:easilyFooled"></d-cite>; here we explore the possibility to backpropagate some objective function, such a feature visualization objective.
    This is easily done since the CPPN network is differentiable as the convolutional neural network and the objective function can be propagated also through the CPPN to update its parameters accordingly.
    That is to say, CPPNs are a differentiable image parameterization --
    a general tool for parameterizing images in any neural art or visualization task.
  </p>

  <figure id="figure-xy2rgb-diagram">
    <%= require("../static/images/diagrams/cppn.svg") %>
    <figcaption>
      <a href="#figure-xy2rgb-diagram" class="figure-number">6</a>:
      CPPNs are a differentiable image parameterization. We can use them for neural art or visualization tasks by backpropagating past the image, through the CPPN to its parameters.
    </figcaption>

  </figure>

  <p>
    Using CPPNs as image parameterization can add an interesting artistic quality to neural art, vaguely reminiscent of light-paintings.<d-footnote>Light-painting is an artistic medium where images are created by manipulating colorful light beams with prisms and mirrors. Notable examples of this technique are the <a href="http://www.lightpaintings.com/">work of Stephen Knapp</a>. <br><br>Note that light-painting metaphor here is rather fragile: for example light composition is an additive process, while CPPNs can have negative-weighted connections between layers.</d-footnote>
    At a more theoretical level, they can be seen as constraining the compositional complexity of your images.
    When used to optimize a feature visualization objective, they produce distinctive images:
  </p>

  <figure id="figure-xy2rgb-examples" class="base-grid">
    <d-figure style="grid-column: screen;" id="CPPN-Examples"></d-figure>
    <figcaption class="add-colab-link--section-xy2rgb" id="figcaption--xy2rgb-examples" style="grid-column: text;">
      <a href="#figure-xy2rgb-examples" class="figure-number">7</a>:
      A <d-cite key="stanley2007cppn">Compositional Pattern Producing Network (CPPN)</d-cite> is used as differentiable parameterization for visualizing features at different layers<d-cite key="olah2017feature"></d-cite>.
    </figcaption>
  </figure>
    <p>
  The visual quality of the generated images is heavily influenced by the architecture of the chosen CPPN.
  Not only the shape of the network, i.e., the number of layers and filters, plays a role, but also the chosen activation functions and normalization. For example, deeper networks produce more fine grained details compared to shallow ones.
  We encourage readers to experiment in generating different images by changing the architecture of the CPPN. This can be easily done by changing the code in the supplementary notebook.
    </p>

    <p>
  The evolution of the patterns generated by the CPPN are artistic artifacts themselves.
  To maintain the metaphor of light-paintings, the optimization process correspond to an iterative adjustments of the beam directions and shapes.
  Because the iterative changes have a more global effect compared to, for example, a pixel parameterization, at the beginning of the optimization only major patterns are visible.
  By iteratively adjusting the weights, our imaginary beams are positioned in such a way that fine details emerge.
    </p>

    <span id="figure-xy2rgb-training" class="figcaption kicker-text-align add-colab-link--section-xy2rgb" style="grid-column: kicker;">
      <p>
        <img class="pointer" src="images/pointer.svg"><br/>
        <a href="#figure-xy2rgb-training" class="figure-number">8</a>:
        <span>Output of CPPNs during training. <em>Control each video by hovering, or tapping it if you are on a mobile device.</em></span>
      </p>
    </span>
    <d-figure id='CPPNAnimations'></d-figure>

     <p>
  By playing with this metaphor, we can also create a new kind of animation that morph one of the above images into a different one.
  Intuitively, we start from one of the light-paintings and we move the beams to create a different one.
  This result is in fact achieved by interpolating the weights of the CPPN representations of the two patterns. A number of intermediate frames are then generated by generating an image given the interpolated CPPN representation.
  As before, changes in the parameter have a global effect and create visually appealing intermediate frames.
    </p>

    <span id="figure-xy2rgb-interpolation" class="figcaption kicker-text-align add-colab-link--section-xy2rgb" style="grid-column: kicker;">
      <p>
        <img class="pointer" src="images/pointer.svg"><br/>
        <a href="#figure-xy2rgb-interpolation" class="figure-number">9</a>:
        Interpolating CPPN weights between two learned points.
      </p>
    </span>


    <d-figure id='CPPNInterpolations'></d-figure>

    <p>
  In this section we presented a parameterization that goes beyond a standard image representation.
  Neural networks, a CPPN in this case, can be used to parameterize an image that is optimized for a given objective function.
  More specifically, we combined a feature-visualization objective function with a CPPN parameterization to create infinite-resolution images of distinctive visual style.
    </p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-rgba" class="section-number">4</a>
  </p>
  <h2 id='section-rgba'><a href="#section-rgba">Generation of Semi-Transparent Patterns</a></h2>

  <p>
    The neural networks used in this article were trained to receive 2D RGB images as input.
    Is it possible to use the same network to synthesize artifacts that span <b> (4) beyond this domain</b>?
    It turns out that we can do so by making our differentiable parameterization define a <em>family</em> of images instead of a single image, and then sampling one or a few images from that family at each optimization step.
    This is important because many of the objects we'll explore optimizing have more degrees of freedom than the images going into the network.
  </p>

  <p>
    To be concrete, let’s consider the case of semi-transparent images. These images have, in addition to the RGB channels, an alpha channel that encodes each pixel's opacity (in the range $[0,1]$). In order to feed such images into a neural network trained on RGB images, we need to somehow collapse the alpha channel. One way to achieve this is to overlay the RGBA image $I$ on top of a background image $BG$ using the standard alpha blending formula <d-footnote>In our experiments we use <a href="https://en.wikipedia.org/wiki/Alpha_compositing#Composing_alpha_blending_with_gamma_correction"> gamma-corrected blending</a> </d-footnote>
  </p>

  <div class="math-overflow" style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
      $O_{rgb} ~~=~~ I_{rgb} * I_a ~~+~~ BG_{rgb} * (1 - I_a)$,
  </div>

  <p>
    where $I_a$ is the alpha channel of the image $I$.
  </p>

  <p>
    If we used a static background $BG$, such as black, the transparency would merely indicate pixel positions in which that background contributes directly to the optimization objective.
    In fact, this is equivalent to optimizing an RGB image and making it transparent in areas where its color matches with the background!
    Intuitively, we'd like transparent areas to correspond to something like "the content of this area could be anything."
    Building on this intuition, we use a different random background at every optimization step.
    <d-footnote>
      We have tried both sampling from real images, and using different types of noise.
      As long as they were sufficiently randomized, the different distributions did not meaningfully influence the resulting optimization.
      Thus, for simplicity, we use a smooth 2D gaussian noise.
    </d-footnote>
  </p>

  <figure id="figure-rgba-diagram" class="subgrid" style="margin-top: 0;">
    <div class="l-body">
      <%= require("../static/images/diagrams/transparency.svg") %>
    </div>
    <!-- <img src="images/diagrams/transparency.svg"/> -->
    <figcaption class="l-body">
      <a href="#figure-rgba-diagram" class="figure-number">10</a>:
      Adding an alpha channel to the image parameterization allows it to represent transparency.
      Transparent areas are blended with a random background at each step of the optimization.
    </figcaption>
  </figure>

  <p>
    By default, optimizing our semi-transparent image will make the image fully opaque, so the network can always get its optimal input.
    To avoid this, we need to change our objective with an objective that encourages some transparency.
    We find it effective to replace the original objective with:
  </p>
  <div class="math-overflow" style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
      $\text{obj}_{\text{new}} ~~=~~ \text{obj}_{\text{old}} ~~*~~ (1-\text{mean}(I_a))$,
  </div>
  <p>
    This new objective automatically balances the original objective $\text{obj}_{\text{old}}$ with reducing the mean transparency.
    If the image becomes very transparent, it will focus on the original objective. If it becomes too opaque, it will temporarily stop caring about the original objective and focus on decreasing the average opacity.
  </p>



  <d-figure class="base-grid" id='SemiTransparentExamples' style="padding: 0; border: none;"></d-figure>
  <p class="l-body figcaption">
    <a href="#figure-rgba-examples" class="figure-number">11</a>:
    Examples of the optimization of semi transparent images for different layers and units.
  </p>

<!--
  <p>
    Feature visualization aims to understand what neurons in a vision model are looking for, by creating images that maximally activate them.
    Unfortunately, there is no way for these visualization to distuingish what the neuron cares about strongly from what it cares about marginally
    <d-footnote>This issue does not occur when optimizing entire channels, because in that case every pixel has multiple neurons that are close to centered on it. As a consequence, the entire input image gets filled with copies of what those neurons care about strongly.</d-footnote>.
    For example, notice the geometric structures around the center of these neuron visualizations:
  </p>
-->

  <p>
    It turns out that the generation of semi-transparent images is useful in feature visualization.
    Feature visualization aims to understand what neurons in a vision model are looking for, by creating images that maximally activate them.
    Unfortunately, there is no way for these visualizations to distinguish which areas of an image strongly influence a neuron's activation and those which only marginally do so.
    <d-footnote>This issue does not occur when optimizing for the activation of entire channels, because in that case every pixel has multiple neurons that are close to centered on it. As a consequence, the entire input image gets filled with copies of what those neurons care about strongly.</d-footnote>
  </p>
  <p>
    Ideally, we would like a way for our visualizations to make this distinction in importance--one natural way to represent that a part of the image doesn't matter is for it to be transparent.
    Thus, if we optimize an image with an alpha channel and encourage the overall image to be transparent, parts of the image that are unimportant according to the feature visualization objective should become transparent.
  </p>

  <d-figure class="subgrid" id="TransparentNeuronExamples"></d-figure>




<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-featureviz-3d" class="section-number">5</a>
  </p>
  <h2 id='section-featureviz-3d'><a href="#section-featureviz-3d">Efficient Texture Optimization through 3D Rendering</a></h2>

  <p>
    In the previous section, we were able to use a neural network for RGB images to create a semi-transparent RGBA image.
    Can we push this even further, creating <b>(4) other kinds of objects</b> even further removed from the RGB input?
    In this section we explore optimizing <b>3D objects</b> for a feature-visualization objective<d-cite key="olah2017feature"></d-cite>.
    We use a 3D rendering process to turn them into 2D RGB images that can be fed into the network, and backpropagate through the rendering process to optimize the texture of the 3D object.
  </p>

  <p>
    Our technique is similar to the approach that Athalye et al.<d-cite key="athalye2017synthesizing"></d-cite> used for the creation of real-world adversarial examples, as we rely on the backpropagation of the objective function to randomly sampled views of the 3D model.
    We differ from existing approaches for artistic texture generation<d-cite key="kato2017neural3D"></d-cite>, as we do not modify the geometry of the object during back-propagation.
    By disentangling the generation of the texture from the position of their vertices, we can create very detailed texture for complex objects.
  </p>

  <p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The object's geometry is usually saved as a collection of interconnected triangles called <b>triangle mesh</b> or, simply, mesh. To render a realistic model, a <b>texture</b> is painted over the mesh. The texture is saved as an image that is applied to the model by using the so called <b>UV-mapping</b>. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>

  <!-- <p>
You can use the following WebGL view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny<d-cite key="turk2005stanfordBunny"></d-cite> and the associated texture<d-cite key="levy2002least"></d-cite>. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the texture is divided in several patches that allows for a complete and undistorted coverage of the object.
  </p>

  <d-figure id="BunnyModel"></d-figure> -->

  <p>
    A simple naive way to create the 3D object texture would be to optimize an image the normal way and then use it as a texture to paint on the object.
    However, this approach generates a texture that does not consider the underlying UV-mapping and, therefore, will create a variety of visual artifacts in the rendered object.
    First, <b>seams</b> are visible on the rendered texture, because the optimization is not aware of the underlying UV-mapping and, therefore, does not optimize the texture consistently along the split patches of the texture.
    Second, the generated patterns are <b>randomly oriented</b> on different parts of the object (see, e.g., the vertical and wiggly patterns) because they are not consistently oriented in the underlying UV-mapping.
    Finally generated patterns are <b>inconsistently scaled</b> because the UV-mapping does not enforce a consistent scale  between triangle areas and their mapped triangle in the texture.
</p>

<d-figure class="base-grid" id="BunnyModelTextureSpaceOptimization"></d-figure>
<p class="l-body figcaption">
  <span class="figure-number-increment"></span>
  <a href="#figure-featureviz-3d-explanation" class="figure-number">13</a>:
  3D model of the famous Stanford Bunny<d-cite key="turk2005stanfordBunny"></d-cite>. You can interact with the model by rotating and zooming. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the render-based optimized texture is divided in several patches that allows for a complete and undistorted coverage of the object.
</p>

  <p>
    We take a different approach.
    Instead of directly optimizing the texture, we optimize the texture <em>through</em> renderings of the 3D object, like those the user would eventually see.
    The following diagram presents an overview of the proposed pipeline:
  </p>

  <figure id="figure-featureviz-3d-diagram">
    <%= require("../static/images/diagrams/featurevis-3d.svg") %>
    <figcaption>
      <a href="#figure-featureviz-3d-diagram" class="figure-number">14</a>:
      We optimize a texture by backpropagating through the rendering process. This is possible because we know how pixels in the rendered image correspond to pixels in the texture.
    </figcaption>
  </figure>

  <p>
We start the process by randomly initializing the texture with a Fourier parameterization.
At every training iteration we sample a random camera position, which is oriented towards the center of the bounding box of the object, and we render the textured object as an image.
We then backpropagate the gradient of the desired objective function, i.e., the feature of interest in the neural network, to the rendered image.
  </p>

  <p>
However, an update of the rendered image does not correspond to an update to the texture that we aim at optimizing. Hence, we need to further propagate the changes to the object's texture.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture.
By modifying the texture, during the following optimization iterations, the rendered image will incorporate the changes applied in the previous iterations.
  </p>

<d-figure id="3DFeatureVizExamples"></d-figure>
<p class="l-body figcaption add-colab-link--section-featureviz-3d">
  <a href="#figure-featureviz-3d-examples" class="figure-number">15</a>:
  Textures are generated by optimizing for a feature visualization objective function.
  Seams in the textures are hardly visible and the patterns are correctly oriented.
</p>

<p>
The resulting textures are consistently optimized along the cuts, hence removing the seams and enforcing an uniform orientation for the rendered object.
Morever, since the function optimization is disentangled by the geometry of the object, the resolution of the texture can be arbitrary high.
In the next section we will see how this framework can be reused for performing an artistic style transfer to the object's texture.
</p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-style-transfer-3d" class="section-number">6</a>
  </p>
  <h2 id='section-style-transfer-3d'><a href="#section-style-transfer-3d">Style Transfer for Textures through 3D Rendering</a></h2>

  <p>
Now that we have established a framework for efficient backpropagation into the UV-mapped texture, we can use it to adapt existing style transfer techniques for 3D objects.
Similarly to the 2D case, we aim at redrawing the original object's texture with the style of a user-provided image.
The following diagram presents an overview of the approach:
  </p>

  <figure class="subgrid" id="figure-style-transfer-3d-diagram">
    <figcaption class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
      <a href="#figure-style-transfer-3d-diagram" class="figure-number">16</a>
    </figcaption>
    <div style="grid-column: text-start / page-end">
      <%= require("../static/images/diagrams/styletransfer-3d.svg") %>
    </div>
  </figure>

  <p>
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture.
At each iteration, we sample a random view point oriented toward the center of the bounding box of the object and we render two images of it: one with the original texture, the <em>content image</em>, and one with the texture that we are currently optimizing, the <em>learned image</em>.
  </p>

  <p>
After the <em>content image</em> and <em>learned image</em> are rendered, we optimize for the style-transfer objective function introduced by Gatys et al.<d-cite key="gatys2015"></d-cite> and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture.
  </p>

  <d-figure class="base-grid" id="3DStyleTransferExamples"></d-figure>
  <p class="l-body figcaption">
    <a href="#figure-style-transfer-3d-examples" class="figure-number">17</a>:
    Style Transfer onto various 3D models. Note that visual landmarks in the content texture, such as eyes, show up correctly in the generated texture.
  </p>

  <p>
Because every view is optimized independently, the optimization is forced to try to add all the style's elements at every iteration.
For example, if we use as style image the Van Gogh's "Starry Night" painting, stars will be added in every single view.
We found we obtain more pleasing results, such as those presented above, by introducing a sort of "memory" of the style of
previous views. To this end, we maintain moving averages of style-representing Gram matrices
over the recently sampled viewpoints. On each optimization iteration we compute the style loss against those averaged matrices,
instead of the ones computed for that particular view.
<d-footnote>
    We use TensorFlow's <code>tf.stop_gradient</code> method to substitute current Gram matrices
    with their moving averages on forward pass, while still propagating the correct gradients
    to the current Gram matrices. <br/><br/>

    An alternative approach, such as the one employed by <d-cite key="kato2017neural3D"></d-cite>,
    would require sampling multiple viewpoints of the scene at each step,
    increasing memory requirements. In contrast, our substitution trick can be also
    used to apply style transfer to high resolution (>10M pixels) images on a
    single consumer-grade GPU.
</d-footnote>
</p>

  <p>
The resulting textures combine elements of the desired style, while preserving the characteristics of the original texture.
Take as an example <a id="3DStyleTransferExamples-VanGogh">the model created by imposing Van Gogh's starry night</a> as style image.
The resulting texture contains the rythmic and vigorous brush strokes that characterize Van Gogh's work.
However, despite the style image's primarily cold tones, the resulting fur has a warm orange undertone as it is preserved from the original texture.
Even more interesting is how the eyes of the bunny are preserved when different styles are transfered.
For example, when the style is obtained from the Van Gogh's painting, the eyes are transformed in a star-like swirl, while <a id="3DStyleTransferExamples-Kandinsky">if Kandinsky's work</a> is used, they become abstract patterns that still resemble the original eyes.
  </p>

  <figure id="figure-style-transfer-3d-picture">
    <img style="width: 100%; border-radius: 4px; object-fit: cover; min-height: 300px;" src="images/printed_bunny_extended.jpg">
    <figcaption>
      <a href="#figure-style-transfer-3d-picture" class="figure-number">18</a>:
      3D print of a style transfer of "<a href="https://www.wikiart.org/en/fernand-leger/the-large-one-parades-on-red-bottom-1953" target="_blank">La grand parade sur fond rouge</a>" (Fernand Leger, 1953) onto the "<a href="http://alice.loria.fr/index.php/software/7-data/37-unwrapped-meshes.html" target="_blank">Stanford Bunny</a>" by Greg Turk & Marc Levoy.
    </figcaption>
  </figure>


<p>
  Textured models produced with the presented method can be easily used with popular 3D modeling software or game engines. To show this, we 3D printed one of the designs as a real-world physical artifact <d-footnote>We used the <a href="https://www.shapeways.com/materials.sandstone">Full-color Sandstone</a> material.</d-footnote>.
</p>

<!-- =================================================== -->

  <hr/>

  <h2 id='conclusions'><a href="#conclusions">Conclusions</a></h2>

  <p>
    For the creative artist or researcher, there's a large space of ways to parameterize images for optimization.
    This opens up not only dramatically different image results, but also animations and 3D objects!
    We think the possibilities explored in this article only scratch the surface.
    For example, one could explore extending the optimization of 3D object textures to optimizing the material or reflectance -- or even go the direction of Kato et al.<d-cite key="kato2017neural3D"></d-cite> and optimize the mesh vertex positions.
  </p>

  <p>
    This article focused on <em>differentiable</em> image parameterizations, because they are easy to optimize and cover a wide range of possible applications.
    But it's certainly possible to optimize image parameterizations that aren't differentiable, or are only partly differentiable, using reinforcement learning or evolutionary strategies <d-cite key="wierstra2014naturalEvolutionStrategies,salimans2017evolution"></d-cite>.
    Using non-differentiable parameterizations could open up exciting possibilities for image or scene generation.
  </p>

  <!-- <p>
    Finally, we think thoughtful use of parameterization can be useful in other optimization problems, providing additional control over the outcome.
    For example, dimensionality reduction is often framed as an optimization problem over the position of points;
    while each point is normally parameterized as a simple position, one could easily use alternate parameterizations.
    This may accelerate dimensionality reduction, or may be a tool in aligning several related dimensionality reductions.
    Of course, people do think very carefully about the way they parameterize the objects they optimize in many domains (eg. neural networks); we just think it's worth considering more generally.
    This kind of more general use of parameterization is in some ways similar to preconditioning and natural gradients.
  </p> -->
  <!-- <aside>WIP, may drop this paragraph</aside> -->


</d-article>



<d-appendix>
  <h3 id="additional-resources" style="letter-spacing: -0.1px;">Additional Resources</h3>
  <p>
    <a href="https://github.com/tensorflow/lucid"><strong>Code:</strong> tensorflow/lucid</a><br>
    Open-source implementation of our techniques on GitHub
  </p>
  <p>
    <a href="https://github.com/tensorflow/lucid#differentiable-image-parameterizations-notebooks"><strong>Notebooks:</strong></a><br>
    Direct links to <code>ipynb</code> notebooks corresponding to the respective sections of this paper.
    <ol style="margin: 0 0 0 30px; padding: 0;">
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/aligned_interpolation.ipynb">Aligned Interpolation</a></li>
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb">Style Transfer beyond VGG</a></li>
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/xy2rgb.ipynb">Compositional Pattern Producing Networks</a></li>
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/transparency.ipynb">Parameterization with Transparency</a></li>
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/texture_synth_3d.ipynb">3D Texture Synthesis</a></li>
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/style_transfer_3d.ipynb">3D Style Transfer</a></li>
    </ol>
  </p>
  <p>
    <a href="https://github.com/tensorflow/lucid/tree/master/notebooks/misc"><strong>Further Notebooks:</strong></a><br>
    Direct links to <code>ipynb</code> notebooks generally related to topics of this paper, or exploring more technical aspects of the techniques described in this paper.
    <ol style="margin: 0 0 0 30px; padding: 0;">
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/appendix/background_experiments.ipynb">Experiments on different backgrounds</a> shows additional experiments from Parameterization with Transparency. Amongst other techniques, contains allied and adversarial backgrounds.</li>
      <li style="margin-bottom: 0.25em;"><a href="https://github.com/tensorflow/lucid/raw/master/notebooks/differentiable-parameterizations/style_transfer_2d.ipynb">Additional Notebook 2</a></li>
    </ol>
  </p>

  
  <h3 id="acknowledgments">Acknowledgments</h3>
  <p>
     We are very grateful to Justin Gilmer, Shan Carter, Dominik Roblek and Amit Patel.
     Justin generously stepped in to handle the review process of this article.
     Shan was available for outstanding design insight and implementation advice.
     Dominik Roblek made the <a href="figure-style-transfer-3d-picture">3D printed example</a> possible, while Amit Patel hunted down an obscure bug in our <a href="figure-style-transfer-diagram">Style Transfer diagram</a>.
  </p>
  <p>
     We appreciate the reviewers who put in the time to write in-depth reviews, which helped us improve our paper significantly: Pang Wei Koh, as well as Anonymous Reviewer 2 and Anonymous Reviewer 3.
     We also thank
     Matt Sharifi, Arvind Satyanarayan, Ian Johnson, and Vincent Sitzmann
     for their thoughts, comments, discussions and support.
  </p>

  <p>
    Lastly, this work was made possible by many open source tools, for which we are grateful.
    In particular, all of our experiments were based on <a href="https://www.tensorflow.org/">Tensorflow</a><d-cite key="abadi2016tensorflow"></d-cite>,
    most of our interactive diagrams are built in <a href="https://svelte.technology/">Svelte</a>,
    and all 3D diagrams use <a href="https://threejs.org/">ThreeJS</a>.
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed the use of transparency, CPPN parameterization, 3D parameterization, and non-VGG style transfer. Chris developed the use of joint parameterization for alignment, and was lightly involved in the development of transparency and 3D models. The final versions presented in the article were refined by Nicola and Ludwig.
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by Nicola and refined by the other authors. The interactive diagrams were designed by Ludwig and Nicola. The final notebooks were primarily created by Ludwig, based on earlier code and notebooks by Alex and Chris.
  </p>

  <h3 id="discussion-review">Discussion and Review</h3>
  <p>
    <a href="https://github.com/distillpub/post--differentiable-parameterizations/issues/50">Review 1 - Pang Wei Koh</a><br>
    <a href="https://github.com/distillpub/post--differentiable-parameterizations/issues/51">Review 2 - Anonymous</a><br>
    <a href="https://github.com/distillpub/post--differentiable-parameterizations/issues/52">Review 3 - Anonymous</a><br>
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
