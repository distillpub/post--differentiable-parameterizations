<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>

<body>

<d-front-matter>
  <script type="text/json">{
  "title": "Differentiable Image Parameterizations",
  "description": "Examples of differentiable image parameterizations that allow solving previously intractable optimization tasks.",
  "password": "params",
  "authors": [
    {
      "author": "Alexander Mordvintsev",
      "authorURL": "https://znah.net/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    },
    {
      "author": "Nicola Pezzotti",
      "authorURL": "https://nicola17.github.io/",
      "affiliation": "Google Research",
      "affiliationURL": "https://research.google.com/"
    },
    {
      "author": "Ludwig Schubert",
      "authorURL": "https://schubert.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    },
    {
      "author": "Chris Olah",
      "authorURL": "https://colah.github.io/",
      "affiliation": "Google Brain Team",
      "affiliationURL": "https://g.co/brain"
    }
  ],
  "katex": {
    "delimiters": [
      {
        "left": "$",
        "right": "$",
        "display": false
      },
      {
        "left": "$$",
        "right": "$$",
        "display": true
      }
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>Differentiable Image Parameterizations</h1>
  <p>A powerful, under-explored tool for neural network visualizations and art.</p>
  <div class="l-page" id="vtoc"></div>
</d-title>

<d-article>


  <p id="first-paragraph">
    Neural networks trained to classify images have a remarkable -- and surprising! -- capacity to generate images.
    Techniques such as DeepDream <d-cite key="mordvintsev2015inceptionism"></d-cite>, style transfer<d-cite key="gatys2015"></d-cite>, and feature visualization<d-cite key="olah2017feature"></d-cite> leverage this capacity as a powerful tool for exploring the inner workings of neural networks, and to fuel a small artistic movement based neural art.
  </p>

  <p>
    All these techniques work in roughly the same way.
    Neural networks used in computer vision have a rich internal representation of the images they look at.
    We can use this representation to describe the properties we want an image to have (e.g. style), and then optimize the input image to have those properties.
    This kind of optimization is possible because the networks are differentiable with respect to their inputs: we can slightly tweak the image to better fit the desired properties, and then iteratively apply such tweaks in gradient descent.
  </p>

  <p>
    Typically, we parameterize the input image as the RGB values of each pixel.
    But that isn’t the only way to parameterize images.
    And, as long as the mapping from parameters to images is differentiable, we can still optimize alternative parameterizations with gradient descent.
  </p>

  <figure class="subgrid" id="figure-differentiable-parameterizations">
    <figcaption style="grid-column: kicker">
      <p>
        <a href="#figure-differentiable-parameterizations" class="figure-number">1</a>:
        <span style="hyphens: manual;">As long as an </span>
        <span style="background-color: #FFF1E7; padding-left: 2px; padding-right: 2px;">image para&shy;meter&shy;ization</span>
        <span>is differ&shy;entiable, we can back&shy;propagate</span>
        <span style="white-space: nowrap;">( <span style="display: inline-block; width: 27px; height: 9px;"> <%= require("../static/images/diagrams/backprop-arrow.svg") %> </span> )</span>
        <span>through it.</span>
      </p>
    </figcaption>
    <div class="l-body">
      <%= require("../static/images/diagrams/general.svg") %>
    </div>
  </figure>

  <p>

    Differentiable image parameterizations invite us to ask "what kind of image generation process can we backpropagate through?"
    The answer is quite a lot, and some of the more exotic possibilities can create a wide range of interesting effects, including 3D neural art, images with transparency, and aligned interpolation.
    Previous work using specific unusual image parameterizations <d-cite key="Nguyen2015:easilyFooled,athalye2017synthesizing,olah2017feature"></d-cite> has shown exciting results -- we think that zooming out and looking at this area as a whole suggests there's even more potential.
  </p>


  <!-- <aside>For each demonstration, we provide a colab notebook so that you can easily reproduce and build on our results.</aside> -->
<!--
  <p>
    Alternative parameterizations can achieve a lot of interesting effects, including 3D neural art, images with transparency, and aligned interpolation.
    We believe
    While recent work has begun to explore some of these possibilities [], we think that there’s a lot of untapped potential
  </p>
-->

<h3>Why Does Parameterization Matter?</h3>


<p>
  It may seem surprising that changing the parameterization of an optimization problem can significantly change the result, despite the objective function that is actually being optimized remaining the same.
  We see four reasons why the choice parameterization can have a significant effect:
</p>

<p><b>(1) - Improved Optimization</b> -
Transforming the input to make an optimization problem easier -- a technique call "preconditioning" -- is a staple of optimization.
  <d-footnote> 
    Preconditioning is most often presented as a transformation of the gradient 
     (usually multiplying it by a positive definite "preconditioner" matrix).
    However, this is equivalent to optimizing an alternate parameterization of the input.
  </d-footnote>
We find that simple changes in parameterization make image optimization for neural art and image optimization much easier.
</p>

<p><b>(2) - Basins of Attraction</b> -
When we optimize the input to a neural network, there are often many different solutions, corresponding to different local minima. 
  <d-footnote> 
    Training deep neural networks characterized by complex optimization landscapes <d-cite key="choromanska2015loss"></d-cite>, which may have many equally good local minima for a given objective.
    (Note that finding the global minimum is not always desirable as it may result in an overfitted model <d-cite key="choromanska2015loss"></d-cite>.)
    Thus, it's probably not surprising that optimizing the input to a neural network would also have many local minima.
  </d-footnote>
The probability of our optimization process falling into any particular local minima is controlled by it's basin of attraction (i.e., the region of the optimization landscape under the influence of the minimum).
Changing the parameterization of an optimization problem is known to change the sizes of different basins of attraction, influencing the likely result.
</p>

<p><b>(3) - Additional Constraints </b> - 
Some parameterizations only parameterize a subset of possible inputs, rather than the entire space.
An optimizer working in such a parameterization will still find solutions that minimize or maximize the objective function, but they'll be subject to the constraints of the parameterization.
By picking the right parameterization, one can impose a variety of constraints, ranging from  simple constraints (e.g., the boundary of the image must be black), to rich, subtle constraints.

<p><b>(4) - Implicitly Optimizing other Objects</b> -
  A parameterization may internally use a different kind of object than the one it outputs and we optimize for.
  For example, while the natural input to a vision network is an RGB image, we can parameterize that image as a rendering of a 3D object and, by backpropagating through the rendering process, optimize that instead.
  Because the 3D object has more degrees of freedom than the image, we generally use a <i>stochastic</i> paramterization that produces images rendered from different perspectives.</p>

<p>
In the rest of the article we give concrete examples were such approaches are beneficial and lead to surprising and interesting visual results.
</p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-aligned-interpolation" class="section-number">1</a>
  </p>
  <h2 id="section-aligned-interpolation"><a href="#section-aligned-interpolation">Aligned Neuron Interpolation</a></h2>

<!--
  <p>
As an introductory example, we explore how <a href="https://distill.pub/2017/feature-visualization/#interpolation">aligned interpolation neuron visualizations</a> are created.
Throughout this article we will often optimize a randomly initialized parameterization to generate the patterns that are detected by a neurons, channels or layers in a neural network.
We refer to the result of this procedure as the optimization for a feature visualizations objective function<d-cite key="olah2017feature"></d-cite>, as it reveals the features that the network is detecting in different layers.
  </p>
-->

  <p>
    Sometimes we'd like to visualize how two neurons interact.
    We can do this by optimizing <a href="https://en.wikipedia.org/wiki/Convex_combination">convex combinations</a> of two neurons.
    If we do this naively, different frames of the resulting visualization will be unaligned--visual landmarks such as eyes appear in different locations in each frame.
    This is because the optimization process that creates the visualization is stochastic: even optimizing for the same objective will lead to the visualization being laid out different each time.
  </p>

  <p>
    Unfortunately, this randomness can make it harder to compare slightly different objectives.
    We can see the issue with independent optimization if we look at the interpolated frames as an animation:
  </p>

  <!-- <img class="pointer" src="images/pointer.svg"> -->
  <span class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#figure-aligned-interpolation-comparison" class="figure-number">2</a>
  </span>
  <d-figure id='AlignedUnalignedComparison'></d-figure>

  <!-- <figure class="">
    <div style="margin-bottom: 1em;" id='UnalignedInterpolation'></div>
    <figcaption>
      Even though all optimized for the same objective, visual features such as eyes appear in different locations in these visualizations.
      This happens because each image starts the optimization from a different, random initialization.
    </figcaption>
  </figure> -->

  <p>
    How can we achieve this aligned interpolation, where visual landmarks do not move between frames?
    There are a number of possible approaches one could try
    <d-footnote>
      For example, one could explicitly penalize differences between adjacent frames. Our final result and our colab notebook use this technique in combination with a shared parameterization.
    </d-footnote>
    , one of which is using a <em>shared parameterization</em>: each frame is parameterized as a combination of it's own unique parameterization, and a single shared parameterization.
    <!-- The shared parameterization makes it easier for the optimization process to find frames in which visual landmarks stay in the same position then frames where they don't. In order to produce an unaligned frame, this frame's unique parameterization would have to overpower the shared parameterization. -->
  </p>

  <span class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#figure-aligned-interpolation-examples" class="figure-number">3</a>
  </span>
  <d-figure class="distill-experimental-autoresize" id='AlignedInterpolationExamples'></d-figure>

  <p>
    By partially sharing a parameterization between frames, we encourage the resulting visualizations to naturally align.
    Intuitively, the shared parameterization provides a common reference for the displacement of visual landmarks, while the unique parameterizations give to each frame its own visual appeal based on its interpolation weights.
    <d-footnote>
      Concretely, we combine a usually lower-resolution shared parameterization $ P_{\text{shared}}$ and full-resolution independent parameterizations $P_{\text{unique}}^i$ that are unique to each frame $i$ of the visualization.
      Each individual frame $i$ is then parameterized as a combination $P^i$ of the two, $P^i = N(P_{\text{unique}}^i + P_{\text{shared}})$, where $N$ is usually the logistic function.
    </d-footnote>
    This parameterization doesn't change the objective, but it does enlarge the <b>(2) basins of attraction</b> where the visualizations are aligned.
    <d-footnote>
      We can explicitly visualize how shared parameterization affects the basins of attraction in a toy example.
      Let's consider optimizing two variables $x$ and $y$ to both minimize $L(z)= (z^2-1)^2$.
      Since $L(z)$ has two basins of attraction $z=1$ or $z=-1$, the pair of optimization problems has four solutions:
      $(x,y) = (1,1)$, $(x,y) = (-1,1)$, $(x,y) = (1,-1)$, or $(x,y) = (-1,-1)$.
      Let's consider randomly initializing $x$ and $y$, and then optimizing them.
      Normally, the optimization problems are independent, so $x$ and $y$ are equally likely to come to unaligned solutions (where they have different signs) as aligned ones.
      But if we add a shared parameterization, the problems become coupled and the basin of attraction where they're aligned becomes bigger.<br>
      <img style="width: 100%; margin-top: 15px; max-width: 500px;" src="images/diagrams/basin-alignment.png"/>
    </d-footnote>
  </p>

  <p>
    Shared parameterizations are an initial example of how differentiable parameterizations in general can be a useful additional tool in visualizing neural networks.
  </p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-styletransfer" class="section-number">2</a>
  </p>
  <h2 id='section-styletransfer'><a href="#section-styletransfer">Style Transfer with non-VGG architectures</a></h2>

  <p>
    Neural style transfer has a mystery:
    despite it's remarkable success, almost all style transfer is done with variants of the <b>VGG architecture</b><d-cite key="simonyan2014vgg"></d-cite>.
    This isn't because no one is interested in doing style transfer on other architectures, but because attempts to do it on other architectures consistently work poorly.
    <d-footnote>
      Examples of experiments performed with different architectures can be found on <a href="https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8">Medium</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/7rrrk3/d_eat_your_vggtables_or_why_does_neural_style/">Reddit</a> and <a href="https://twitter.com/hardmaru/status/954173051330904065">Twitter</a>.
    </d-footnote>
    <!--This has practical implications: for example, if we could get style transfer working on smaller networks, using it in mobile applications would become more feasible.-->
  </p>

  <p>
    Several hypotheses have been proposed to explain why VGG works so much better than other models.
    One suggested explanation is that VGG's large size causes it to capture information that other models discard.
    This extra information, the hypothesis goes, isn't helpful for classification, but it does cause the model to work better for style transfer.
    An alternate hypothesis is that other models downsample more aggressively than VGG, losing spatial information.
    We suspect that there may be another factor: most modern vision models have checkerboard artifacts in their gradient <d-cite key="odena2016deconvolution,olah2017feature"></d-cite>, which could make optimization of the stylized image more difficult.
  </p>

  <p>
    We find that the way we parameterize the image is just as important as the model we use for style transfer, hence acting as a preconditioner for the problem and helping in <b>(1) improving the optmization</b>.
    For example, with the right image parameterization, GoogLeNet<d-cite key="szegedy2015googlenet"></d-cite> can produce images of comparable quality to VGG.
  </p>

  <d-figure id='StyleTransferExamples' class="base-grid"></d-figure>

  <p class="figcaption l-body">
    <a href="#figure-style-transfer-examples" class="figure-number">4</a>:
    Move the slider under "final image optimization" to compare optimization in pixel space and in the fourier basis. Both images were created with the same optimization objective and differ only in their parameterization.
  </p>

  <p>
    We use the same decorrelated parameterization
    <d-footnote>
      <span>For images, this means that the gradient descent must not be performed in the pixel space, but rather in the space of the Fourier basis where frequencies are scaled so that they all have equal energy. </span><br/><br/>
      <span>A reference implementation of this parameterization is available in our visualization library <a href="https://github.com/tensorflow/lucid"  target="_blank">lucid</a> as <a href="https://github.com/tensorflow/lucid/blob/a40d0e411eee8c18a2e2ca1958383049354478c3/lucid/optvis/param/spatial.py#L38" target="_blank"><code>param.fft_image</code></a>. We have found it robust enough that it is the default behavior of <code>param.image</code>.</span>
    </d-footnote>
    and transformation robustness
    <d-footnote>
      Transformation robustness is achieved by stochastically jittering, rotating and scaling the image before applying the optimization step.
    </d-footnote>
    that Olah et al.<d-cite key="olah2017feature"></d-cite> found helpful for feature visualization.

  </p>

  <figure id="figure-style-transfer-diagram" class="subgrid">
    <!-- <img src="images/diagrams/styletransfer.png"/> -->
    <figcaption class="kicker-text-align" style="grid-column: kicker;">
      <a href="#figure-style-transfer-diagram" class="figure-number">5</a>
      <!-- Style Transfer optimizes an image for two objectives: a <strong>content objective</strong>, which aims to get neurons to fire in the same position as they did for the content image, and a <strong>style objective</strong>, which aims to create similar patterns of neuron activation anywhere as in the style image without regard to position.
      Normally, this process is sensitive to the choice of model. By parameterizing the optimized image in a decorrelated space it works for many more models. -->
    </figcaption>
    <div style="grid-column: text-start / page-end;">
      <%= require("../static/images/diagrams/styletransfer.svg") %>
    </div>
  </figure>

  <p>
    When we use this parameterization, we find that the style transfer objective proposed by Gatys<d-cite key="gatys2015"></d-cite> produces convincing results even on architectures other than VGG.
    Picking the right parameterization can be the difference between techniques like style transfer working and failing.
  </p>


  <!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-xy2rgb" class="section-number">3</a>
  </p>
  <h2 id='section-xy2rgb'><a href="#section-xy2rgb">Compositional Pattern Producing Networks</a></h2>

  <p>
    So far, we’ve explored image parameterizations that are relatively close to how we normally think of images, using pixels or Fourier components.
    In this section, we explore the possibility of <b> (3) adding additional constraints</b> to the optimization process by using a different parameterization.
    More specifically, we parameterize our image as a neural network -- in particular, a Compositional Pattern Producing Network (CPPN) <d-cite key="stanley2007cppn"></d-cite>.
  </p>

  <p>
    CPPNs are neural networks that map $(x,y)$ positions to image colors:
  </p>
  <div class="math-overflow" style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
    $(x,y) ~\xrightarrow{\tiny CPPN}~ (r,g,b)$
  </div>
  <p>
    By applying the CPPN to a grid of positions, one can make arbitrary resolution images.
    The parameters of the CPPN network -- the weights and biases -- determine what image is produced.
    Depending on the architecture chosen for the CPPN, pixels in the resulting image are constraint to share, up to a certain degree, the color of their neighbors.
  </p>

  <p>
    Random parameters can produce aesthetically interesting images <d-cite key="CPPN:random"></d-cite>, but we can produce more interesting images by learning the parameters of the CPPN.
    Often this is done by evolution  <d-cite key="stanley2007cppn,Nguyen2015:easilyFooled"></d-cite>; here we explore the possibility to backpropagate some objective function, such a feature visualization objective.
    This is easily done since the CPPN network is differentiable as the convolutional neural network and the objective function can be propagated also trough the CPPN to update its parameters accordingly.
    That is to say, CPPNs are a differentiable image parameterization --
    a general tool for parameterizing images in any neural art or visualization task.
  </p>

  <figure id="figure-xy2rgb-diagram">
    <%= require("../static/images/diagrams/cppn.svg") %>
    <figcaption>
      <a href="#figure-xy2rgb-diagram" class="figure-number">6</a>:
      CPPNs are a differentiable image parameterization. We can use them for neural art or visualization tasks by backpropagating past the image, through the CPPN to its parameters.
    </figcaption>

  </figure>

  <p>
    Using CPPNs as image parameterization can add an interesting artistic quality to neural art, vaguely reminiscent of light-paintings.<d-footnote>Light-painting is an artistic medium where images are created by manipulating colorful light beams with prisms and mirrors. Notable examples of this technique are the <a href="http://www.lightpaintings.com/">work of Stephen Knapp</a>.</d-footnote>
    At a more theoretical level, they can be seen as constraining the compositional complexity of your images.
    When used to optimize a feature visualization objective, they produce distinctive images:
  </p>

  <figure id="figure-xy2rgb-examples" class="base-grid">
    <d-figure style="grid-column: screen;" id="CPPN-Examples"></d-figure>
    <figcaption style="grid-column: text;">
      <a href="#figure-xy2rgb-examples" class="figure-number">7</a>:
      A <d-cite key="stanley2007cppn">Compositional Pattern Producing Network (CPPN)</d-cite> is used as differentiable parameterization for visualizing features at different layers<d-cite key="olah2017feature"></d-cite>.
    </figcaption>
  </figure>
    <p>
  The visual quality of the generated images is heavily influenced by the architecture of the chosen CPPN.
  Not only the shape of the network, i.e., the number of layers and filters, plays a role, but also the chosen activation functions and normalization.  For example, deeper networks produce more fine grained details compared to shallow ones.
  We encourage readers to experiment in generating different images by changing the architecture of the CPPN. This can be easily done by changing the code in the supplementary notebook.
    </p>

    <p>
  The evolution of the patterns generated by the CPPN are artistic artifacts themselves.
  To maintain the metaphor of light-paintings, the optimization process correspond to an iterative adjustements of the beam directions and shapes.
  Because the iterative changes have a more global effect compared to, for example, a pixel parameterization, at the beginning of the optimization only major patterns are visible.
  By iteratively adjusting the weights, our immaginary beams are positioned in such a way that fine details emerge.
    </p>

    <span id="figure-xy2rgb-training" class="figcaption kicker-text-align" style="grid-column: kicker;">
      <img class="pointer" src="images/pointer.svg"><br/>
      <a href="#figure-xy2rgb-training" class="figure-number">8</a>:
      <span>Output of CPPNs during training. <em>Control each video by hovering, or tapping it if you are on a mobile device.</em></span>
    </span>
    <d-figure id='CPPNAnimations'></d-figure>

     <p>
  By playing with this metaphor, we can also create a new kind of animation that morph one of the above images into a different one.
  Intuitively, we start from one of the light-paintings and we move the beams to create a differrent one.
  This result is in fact achieved by interpolating the weights of the CPPN representations of the two patterns. A number of intermediate frames are then generated by generating an image given the interpolated CPPN representation.
  As before, changes in the parameter have a global effect and create visually appealing intermediate frames.
    </p>

    <span id="figure-xy2rgb-interpolation" class="figcaption kicker-text-align" style="grid-column: kicker;">
      <img class="pointer" src="images/pointer.svg"><br/>
      <a href="#figure-xy2rgb-interpolation" class="figure-number">9</a>:
      Interpolating CPPN weights between two learned points.
    </span>


    <d-figure id='CPPNInterpolations'></d-figure>

    <p>
  In this section we presented a parameterization that goes beyond a standard image representation.
  Neural networks, a CPPN in this case, can be used to parameterize an image that is optimized for a given objective function.
  More specifically, we combined a feature-visualization objective function with a CPPN parameterization to create infinite-resolution images of distinctive visual style.
    </p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-rgba" class="section-number">4</a>
  </p>
  <h2 id='section-rgba'><a href="#section-rgba">Generation of Semi-Transparent Patterns</a></h2>

  <p>
    The neural networks used in this article were trained to receive 2D RGB images as input.
    Is it possible to use the same network to synthesize artifacts that span <b> (4) beyond this domain</b>?
    It turns out that we can do so by making our differentiable parameterization define a <i>family</i> of images instead of a single image, and then sampling one or a few images from that family at each optimization step.
    This is important because many of the objects we'll explore optimizing have more degrees of freedom than the images going into the network.
  </p>

  <p>
    To be concrete, let’s consider the case of semi-transparent images. These images have, in addition to the RGB channels, an alpha channel that encodes pixel's opacity (in the range $[0,1]$). In order to feed such images into a neural network trained on RGB images, we need to somehow collapse the alpha channel. One way to achieve this is to overlay the RGBA image $I$ on top of a background image $BG$ using the standard alpha blending formula <d-footnote>In our experiments we use <a href="https://en.wikipedia.org/wiki/Alpha_compositing#Composing_alpha_blending_with_gamma_correction"> gamma-corrected blending</a> </d-footnote>
  </p>

  <div class="math-overflow" style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
      $O_{rgb} ~~=~~ I_{rgb} * I_a ~~+~~ BG_{rgb} * (1 - I_a)$,
  </div>

  <p>
    where $I_a$ is the alpha channel of the image $I$.
  </p>
  
  <p>
    If we used a static background $BG$, such as black, the transparency would merely indicate pixel positions in which that background contributes directly to the optimization objective.
    In fact, this is equivalent to optimizing an RGB image and making it transparent in areas where its color matches with the background!
    Intuitively, we'd like transparent areas to correspond to something like "the content of this area could be anything."
    Building on this intuition, we use a different random background at every optimization step.
    <d-footnote>
      We have tried both sampling from real images, and using different types of noise.
      As long as they were sufficiently randomized, the different distributions did not meaningfully influence the resulting optimization.
      Thus, for simplicity, we use a smooth 2D gaussian noise.<br><br>
      Another possibility is to use an adversarial background! We initially thought this was a clever, principled idea—but it, too, produced very similar results to the default noise.
      TODO: example image <img>
    </d-footnote>
  </p>
  
  <figure id="figure-rgba-diagram" class="subgrid" style="margin-top: 0;">
    <div class="l-body">
      <%= require("../static/images/diagrams/transparency.svg") %>
    </div>
    <!-- <img src="images/diagrams/transparency.svg"/> -->
    <figcaption class="l-body">
      <a href="#figure-rgba-diagram" class="figure-number">10</a>:
      Adding an alpha channel to the image parameterization allows it to represent transparency.
      Transparent areas are blended with a random background at each step of the optimization.
    </figcaption>
  </figure>
  
  <p>
    By default, optimizing our semi-transparent image will make the image fully opaque, so the network can always get it's optimal input.
    To avoid this, we need to change our objective with an objective that encourages some transparency.
    We find it effective to replace the original objective with:
  </p>
  <div class="math-overflow" style="margin-right: auto; margin-left: auto; margin-top: 0px; margin-bottom: 20px;">
      $\text{obj}_{\text{new}} ~~=~~ \text{obj}_{\text{old}} ~~*~~ (1-\text{mean}(I_a))$,
  </div>
  <p>
    This new objective automatically balances the original objective $\text{obj}_{\text{old}}$ with reducing the mean transparency.
    If the image becomes very transparent, it will focus on the original objective. If it becomes too opaque, it will temporarily stop caring about the original objective and focus on decreasing the average opacity.
  </p>



  <d-figure class="base-grid" id='SemiTransparentExamples' style="padding: 0; border: none;"></d-figure>
  <p class="l-body figcaption">
    <a href="#figure-rgba-examples" class="figure-number">11</a>:
    Examples of the optimization of semi transparent images for different layers and units.
  </p>

<!--
  <p>
    Feature visualization aims to understand what neurons in a vision model are looking for, by creating images that maximally activate them.
    Unfortunately, there is no way for these visualization to distuingish what the neuron cares about strongly from what it cares about marginally
    <d-footnote>This issue does not occur when optimizing entire channels, because in that case every pixel has multiple neurons that are close to centered on it. As a consequence, the entire input image gets filled with copies of what those neurons care about strongly.</d-footnote>.
    For example, notice the geometric structures around the center of these neuron visualizations:
  </p>
-->

  <p>
    It turns out that the generation of semi-transparent images is useful in feature visualization.
    Feature visualization aims to understand what neurons in a vision model are looking for, by creating images that maximally activate them.
    Unfortunately, there is no way for these visualizations to distuingish what areas of an image a neuron cares about strongly from what it cares about marginally.
    <d-footnote>This issue does not occur when optimizing entire channels, because in that case every pixel has multiple neurons that are close to centered on it. As a consequence, the entire input image gets filled with copies of what those neurons care about strongly.</d-footnote>
    Ideally, we would like a way for our visualizations to make this distinction in importance--one natural way to represent that a part of the image doesn't matter is for it to be transparent.
    Thus, if we optimize an image with an alpha channel and encourage the image to be transparent, unimportant parts of the image, according to the feature visualization objective, should become transparent.
  </p>

  <d-figure class="subgrid" id="TransparentNeuronExamples"></d-figure>




<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-featureviz-3d" class="section-number">5</a>
  </p>
  <h2 id='section-featureviz-3d'><a href="#section-featureviz-3d">Efficient Texture Optimization through 3D Rendering</a></h2>

  <p>
    In the previous section, we were able to use a neural network for RGB images to create a semi-transparent RGBA image.
    Can we push this even further, creating <b>(4) other kinds of objects</b> even further removed from the RGB input?
    In this section we explore optimizing <b>3D objects</b> for a feature-visualization objective<d-cite key="olah2017feature"></d-cite>.
    We use a 3D rendering process to turn them into 2D RGB images that can be fed into the network, and backpropegate through the rendering process to optimize the texture of the 3D object.
  </p>
  
  <p>
    Our technique is similar to the approach that Athalye et al.<d-cite key="athalye2017synthesizing"></d-cite> used for the creation of real-world adversarial examples, as we rely on the backpropagation of the objective function to randomly sampled views of the 3D model.
    We differ from existing approaches for artistic texture generation<d-cite key="kato2017neural3D"></d-cite>, as we do not modify the geometry of the object during back-propagation.
    By disentangling the generation of the texture from the position of their vertices, we can create very detailed texture for complex objects.
  </p>

  <p>
Before we can describe our approach, we first need to understand how a 3D object is stored and rendered on screen. The object's geometry is usually saved as a collection of interconnected triangles called <b>triangle mesh</b> or, simply, mesh. To render a realistic model, a <b>texture</b> is painted over the mesh. The texture is saved as an image that is applied to the model by using the so called <b>UV-mapping</b>. Every vertex $c_i$ in the mesh is associated to a $(u_i,v_i)$ coordinate in the texture image. The model is then rendered, i.e. drawn on screen, by coloring every triangle with the region of the image that is delimited by the $(u,v)$ coordinates of its vertices.
  </p>

  <!-- <p>
You can use the following WebGL view to familiarize with these concepts. The view shows the 3D model of the famous Stanford Bunny<d-cite key="turk2005stanfordBunny"></d-cite> and the associated texture<d-cite key="levy2002least"></d-cite>. You can interact with the model by rotating, zooming and panning. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the texture is divided in several patches that allows for a complete and undistorted coverage of the object.
  </p>

  <d-figure id="BunnyModel"></d-figure> -->

  <p>
    A simple naive way to create the 3D object would be to optimize an image the normal way and then use it as a texture to paint on the object.
    However, this approach generates a texture that does not consider the underlying UV-mapping and, therefore, will create a variety of visual artifacts in the rendered object.
    First, <b>seams</b> are visible on the rendered texture, because the optimization is not aware of the underlying UV-mapping and, therefore, does not optimize the texture consistently along the split patches of the texture.
    Second, the generated patterns are <b>randomly oriented</b> on different parts of the object (see, e.g., the vertical and wigly patterns) because they are not consistently oriented in the underlying UV-mapping.
    Finally generated patterns are <b>inconsistently scaled</b> because the UV-mapping does not enforce a consistent scale  between triangle areas and their mapped triangle in the texture.
</p>

<d-figure class="base-grid" id="BunnyModelTextureSpaceOptimization"></d-figure>
<p class="l-body figcaption">
  <span class="figure-number-increment"></span>
  <a href="#figure-featureviz-3d-explanation" class="figure-number">13</a>:
  3D model of the famous Stanford Bunny<d-cite key="turk2005stanfordBunny"></d-cite>. You can interact with the model by rotating and zooming. Moreover, you can unfold the object to its two-dimensional texture representation. This unfolding reveals the UV mapping used to store the texture in the texture image. Note how the render-based optimized texture is divided in several patches that allows for a complete and undistorted coverage of the object.
</p>

  <p>
    We take a different approach.
    Instead of directly optimizing the texture, we optimize the texture <i>through</i> renderings of the 3D object, like those the user would eventually see.
    The following diagram presents an overview of the proposed pipeline:
  </p>

  <figure id="figure-featureviz-3d-diagram">
    <%= require("../static/images/diagrams/featurevis-3d.svg") %>
    <figcaption>
      <a href="#figure-featureviz-3d-diagram" class="figure-number">14</a>:
      We optimize a texture by backpropagating through the rendering process. This is possible because we know how pixels in the rendered image correspond to pixels in the texture.
    </figcaption>
  </figure>

  <p>
We start the process by randomly initializing the texture with a Fourier parameterization.
At every training iteration we sample a random camera position, which is oriented towards the center of mass of the object, and we render the textured object as an image.
We then backpropagate the gradient of the desired objective function, i.e., the feature of interest in the neural network, to the rendered image.
  </p>

  <p>
However, an update of the rendered image does not correspond to an update to the texture that we aim at optimizing. Hence, we need to further propagate the changes to the object's texture.
The propagation is easily implemented by applying a reverse UV-mapping, as for each pixel on screen we know its coordinate in the texture.
By modifying the texture, during the following optimization iterations, the rendered image will incorporate the changes applied in the previous iterations.
  </p>

<d-figure class="base-grid" id="3DFeatureVizExamples"></d-figure>
<p class="l-body figcaption">
  <a href="#figure-featureviz-3d-examples" class="figure-number">15</a>:
  Textures are generated by optimizing for a feature visualization objective function.
  Seams in the textures are hardly visible and the patterns are correctly oriented.
</p>

<p>
The resulting textures are consistently optimized along the cuts, hence removing the seams and enforcing an uniform orientation for the rendered object.
Morever, since the function optimization is disentangled by the geometry of the object, the resolution of the texture can be arbitrary high.
In the next section we will se how this framework can be reused for performing an artistic style transfer to the object's texture.
</p>

<!-- =================================================== -->

  <hr/>
  <p class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
    <a href="#section-style-transfer-3d" class="section-number">6</a>
  </p>
  <h2 id='section-style-transfer-3d'><a href="#section-style-transfer-3d">Style Transfer for Textures through 3D Rendering</a></h2>

  <p>
Now that we have established a framework for efficient backpropagation into the UV-mapped texture, we can use it to adapt existing style transfer techniques for 3D objects.
Similarly to the 2D case, we aim at redrawing the original object's texture with the style of a user-provided image.
The following diagram presents an overview of the approach:
  </p>

  <figure class="subgrid" id="figure-style-transfer-3d-diagram">
    <figcaption class="figcaption kicker-text-align" style="grid-column: kicker; margin-top: 20px;">
      <a href="#figure-style-transfer-3d-diagram" class="figure-number">16</a>
    </figcaption>
    <div style="grid-column: text-start / page-end">
      <%= require("../static/images/diagrams/styletransfer-3d.svg") %>
    </div>
  </figure>

  <p>
The algorithm works in similar way to the one presented in the previous section, starting from a randomly initialized texture.
At each iteration, we sample a random view point oriented toward the center of mass of the object and we render two images of it, one with the original texture and one with the texture that we are currently optimizing.
  </p>

  <p>
After the <em>content image</em> and <em>learned image</em> are rendered, we optimize for the style-transfer objective function introduced by Gatys et al.<d-cite key="gatys2015"></d-cite> and we map the parameterization back in the UV-mapped texture as introduced in the previous section.
The procedure is then iterated until the desired blend of content and style is obtained in the target texture.
  </p>

  <d-figure class="base-grid" id="3DStyleTransferExamples"></d-figure>
  <p class="l-body figcaption">
    <a href="#figure-style-transfer-3d-examples" class="figure-number">17</a>:
    Style Transfer onto various 3D models. Note that visual landmarks in the content texture, such as eyes, show up correctly in the generated texture.
  </p>

  <p>
Because every view is optimized independently, the optimization is forced to add all the style elements at every iteration.
For example, if we use as style image the Van Gogh's "Starry Night" painting, stars will be added in every single view.
We found that more pleasing results, as those presented above, are obtained by introducing a sort of "memory" of the style of
previous views. To this end, we maintain moving averages of style-representing Gram matrices <d-cite key="gatys2015"></d-cite>
over the recently sampled viewpoints. On each optimization iteration we compute style loss against those averaged matrices,
instead of the ones computed for the particular view.
  </p>

  <p>
The resulting textures combine elements models of the desired style, while preserving the characteristics of the original texture.
Take as an example the model created by imposing Van Gogh's starry night as style image.
The resulting texture contains the repetitive and vigorous brush strokes that characterize Van Gogh's work.
However, despite the style image contains only cold tones, the resulting fur has a warm orange undertone as it is preserved from the original texture.
Even more interesting is how the eyes of the bunny are preserved when different styles are transfered.
For example, when the style is obtained from the Van Gogh's painting, the eyes are transformed in a star-like swirl, while if Kandinsky's work is used, they become abstract patterns that still resemble the original eyes.
  </p>

  <figure id="figure-style-transfer-3d-picture">
    <img style="width: 100%; border-radius: 4px;" src="images/printed_bunny_extended.jpg">
    <figcaption>
      <a href="#figure-style-transfer-3d-picture" class="figure-number">18</a>:
      3D print of a style transfer of "<a href="https://www.wikiart.org/en/fernand-leger/the-large-one-parades-on-red-bottom-1953" target="_blank">The large one parades on red bottom</a>" (Fernand Leger, 1953) onto the "<a href="http://alice.loria.fr/index.php/software/7-data/37-unwrapped-meshes.html" target="_blank">Stanford Bunny</a>" by Greg Turk & Marc Levoy.
    </figcaption>
  </figure>


<p>
Our approach combines well with modern digital fabrication techniques<d-cite key="gershenfeld2012make,kodama1981automatic,jones2011reprap"></d-cite>.
The geometry of the object is not part of its parameterization and, therefore, it is not modified during the optimization process.
Hence, a faithful replica of the object can be 3D printed and, by adopting the latest color printing technologies<d-footnote>Full color sandstone was used to print and color our model.</d-footnote>, it can be textured accordingly to our generated style texture.
</p>

<!-- =================================================== -->

  <hr/>
  
  <h2 id='conclusions'><a href="#conclusions">Conclusions</a></h2>

  <p>
    For the creative artist or researcher, there's a large space of ways to parameterize images for optimization.
    This opens up not only dramatically different image results, but also animations and 3D objects!
    We think the possibilities explored in this article only scratch the surface.
    For example, one could explore extending the optimization of 3d object textures to optimizing the material or reflectance -- or even go the direction of Kato et al.<d-cite key="kato2017neural3D"></d-cite> and optimize the mesh vertex positions.
  </p>

  <p>
    This article focused on <i>differentiable</i> image parameterizations, because they are easy to optimize and cover a wide range of possible applications.
    But it's certain possible to optimize image parameterizations that aren't differentiable, or are only partly differentiable, using reinforcement learning or evolutionary strategies <d-cite key="wierstra2014naturalEvolutionStrategies,salimans2017evolution"></d-cite>.
    Using non-differentiable parameterizations could open up exciting possibilities for image or scene parameterization.
  </p>

  <p>

  </p>

  <p>
    Finally, we think thoughtful use of parameterization can be useful in other optimization problems, providing additional control over the outcome.
    For example, dimensionality reduction is often framed as an optimization problem over the position of points;
    while each point is normally parameterized as a simple position, one could easily use alternate parameterizations.
    This may accelerate dimensionality reduction, or may be a tool in aligning several related dimensionality reductions.
    Of course, people do think very carefully about the way they parameterize the objects they optimize in many domains (eg. neural networks); we just think it's worth considering more generally.
    This kind more general use of parameterization is in some ways similar to preconditioning and natural gradients.
  </p>
  <!-- TODO: remove? -->
  <aside>WIP, may drop this paragraph</aside>


</d-article>



<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to Shan Carter for outstanding design advice. We also really appreciate the support of Matt Sharifi. Finally, the 3D printed example was made possible by Dominik Roblek.
  </p>

  <p>
    Many of our diagrams are based on diagrams in previous Distill articles, especially <a href="https://distill.pub/2017/feature-visualization/">Feature Visualization</a>.
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed the use of transparency, CPPN parameterization, 3D parameterization, and non-VGG style transfer. Chris developed the use of joint parameterization for alignment, and was lightly involved in the development of transparency and 3D models. The final versions presented in the article were refined by Nicola and Ludwig.
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by Nicola and refined by the other authors. The interactive diagrams were designed by Ludwig and Nicola. The final notebooks were primarily created by Ludwig, based on earlier code and notebooks by Alex and Chris.
  </p>


  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="bibliography.bib"></d-bibliography>

</body>
